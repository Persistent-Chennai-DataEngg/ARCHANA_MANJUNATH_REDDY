{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b7d3da2-1ad4-4adc-891f-42d2cec3f432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+------+------+\n",
      "|name|age|  Role|salary|\n",
      "+----+---+------+------+\n",
      "|John| 24|    HR| 50000|\n",
      "| Ria| 22|   DEv|450000|\n",
      "|Dave| 26|Devops| 55000|\n",
      "+----+---+------+------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Role: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dataframe\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import col, avg, max\n",
    "spark = SparkSession.builder.appName(\"DemoApp\").getOrCreate()\n",
    "data = [(\"John\",24,\"HR\", 50000), (\"Ria\", 22, \"DEv\", 450000), (\"Dave\", 26,\"Devops\", 55000)]\n",
    "#inferschema \n",
    "schema = StructType([StructField(\"name\", StringType(), True), StructField(\"age\", IntegerType(), True), \n",
    "                     StructField(\"Role\", StringType(), True), StructField(\"salary\", IntegerType(), True)])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "#df_csv = spark.read.csv(\"./Documents/users.csv\", header = True, inferSchema = True)\n",
    "#df_csv.show()\n",
    "#df_csv.select(\"name\",\"age\").filter(df_csv.age > 18).show()\n",
    "#df_updated = df_csv.withColumn(\"retirement\",60 - col(\"age\"))\n",
    "#df.withColumnRenamed(\"\",\"\")\n",
    "#df.drop(\"location\")\n",
    "#df_updated.show()\n",
    "#df_csv.groupBy(\"location\").agg(avg(\"age\").alias(\"Average_Age\"), max(\"age\").alias(\"Maximum Age\")).show()\n",
    "roles = [(\"max\",\"developer\"),(\"john\",\"hr\"),(\"ria\",\"UI/UX\"), (\"max\", \"devops\"), (\"david\", \"QA\")]\n",
    "schema = StructType([StructField(\"name\", StringType(), True), StructField(\"role\", StringType(), True)])\n",
    "df_roles = spark.createDataFrame(roles, schema)\n",
    "#df_joined = df_csv.join(df_roles, \"name\", \"inner\")\n",
    "#df_joined.show()\n",
    "#df.dropna()\n",
    "#df.fillna({\"location\":\"Chennai\"})\n",
    "#df_csv.createOrReplaceTempView(\"people\")\n",
    "#res = spark.sql(\"SELECT name, age FROM people WHERE age > 18\")\n",
    "#res.orderBy(\"age\", ascending = False).show()\n",
    "#res.write.csv(\"./results.csv\", mode = \"overwrite\")\n",
    "df.show()\n",
    "df.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587e31f9-5f17-457c-b408-13fb7b768489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2ca6b78-156d-4d31-929e-8134b5346915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------------+\n",
      "|     house|year|total_points|\n",
      "+----------+----+------------+\n",
      "| Slytherin|   3|          85|\n",
      "|Gryffindor|   2|          90|\n",
      "| Slytherin|   2|          70|\n",
      "|Hufflepuff|   2|          65|\n",
      "| Ravenclaw|   2|          55|\n",
      "|Gryffindor|   1|          80|\n",
      "| Slytherin|   1|          60|\n",
      "+----------+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Problem 1: Hogwarts House Points Analysis \n",
    "Problem Description:\n",
    "At Hogwarts School of Witchcraft and Wizardry, the four houses (Gryffindor, Hufflepuff, Ravenclaw, Slytherin) \n",
    "earn points for their achievements throughout the year. You’ve been tasked by Professor Dumbledore to analyze \n",
    "the house points data to determine which house is leading and summarize points by house and year. \n",
    "Select the relevant columns (house, year, points), filter for houses with more than 50 points in a \n",
    "given year, group by house and year to sum the points, and order the results by year \n",
    "and total points in descending order to see who’s dominating the House Cup race. \n",
    "The Great Hall is buzzing with anticipation for these results.\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum\n",
    "spark = SparkSession.builder.appName(\"HogwartsAnalysis\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"Gryffindor\", 1, 80, \"Harry Potter\"),\n",
    "    (\"Slytherin\", 1, 60, \"Draco Malfoy\"),\n",
    "    (\"Ravenclaw\", 1, 45, \"Luna Lovegood\"),\n",
    "    (\"Hufflepuff\", 1, 30, \"Cedric Diggory\"),\n",
    "    (\"Gryffindor\", 2, 90, \"Hermione Granger\"),\n",
    "    (\"Slytherin\", 2, 70, \"Pansy Parkinson\"),\n",
    "    (\"Ravenclaw\", 2, 55, \"Cho Chang\"),\n",
    "    (\"Hufflepuff\", 2, 65, \"Hannah Abbott\"),\n",
    "    (\"Gryffindor\", 3, 20, \"Ron Weasley\"),\n",
    "    (\"Slytherin\", 3, 85, \"Blaise Zabini\")\n",
    "]\n",
    "\n",
    "columns = [\"house\", \"year\", \"points\", \"student\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "res=(df.select(\"house\",\"year\",\"points\").filter(df.points>50).groupBy(\"house\",\"year\").agg(sum(\"points\").alias(\"total_points\")).orderBy(\"year\",\"total_points\", ascending=False))\n",
    "res.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd56af9d-221d-4a1f-a8b5-407c6117a3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+\n",
      "|  race|      avg_enemies|\n",
      "+------+-----------------+\n",
      "|   Elf|             17.5|\n",
      "|Wizard|             12.0|\n",
      "| Human|8.666666666666666|\n",
      "| Dwarf|              8.0|\n",
      "+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Problem 2: Fellowship of the Ring Battle Stats \n",
    "Problem Description:\n",
    "The Fellowship of the Ring is battling across Middle-earth, and Gandalf needs your help \n",
    "to analyze their combat performance. The data tracks each member’s battles, enemies defeated, and injuries sustained.\n",
    "To prepare for the next council in Rivendell, select the member’s name, race, and enemies defeated, \n",
    "filter for battles where more than 5 enemies were defeated, group by race to calculate the average enemies defeated,\n",
    "and order by the average in descending order. This will help the council decide which races are most effective in\n",
    "combat and plan their strategy against Sauron’s forces.\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "spark = SparkSession.builder.appName(\"RingBattleStats\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"Aragorn\", \"Human\", 10, 2, \"Helms Deep\"),\n",
    "    (\"Legolas\", \"Elf\", 15, 0, \"Helms Deep\"),\n",
    "    (\"Gimli\", \"Dwarf\", 8, 3, \"Helms Deep\"),\n",
    "    (\"Frodo\", \"Hobbit\", 2, 1, \"Moria\"),\n",
    "    (\"Sam\", \"Hobbit\", 4, 2, \"Moria\"),\n",
    "    (\"Gandalf\", \"Wizard\", 12, 1, \"Moria\"),\n",
    "    (\"Boromir\", \"Human\", 7, 4, \"Amon Hen\"),\n",
    "    (\"Legolas\", \"Elf\", 20, 0, \"Amon Hen\"),\n",
    "    (\"Aragorn\", \"Human\", 9, 2, \"Amon Hen\")\n",
    "]\n",
    "\n",
    "columns = [\"name\", \"race\", \"enemies_defeated\", \"injuries\", \"battle\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "res1=(df.select(\"name\",\"race\",\"enemies_defeated\").filter(df.enemies_defeated > 5).groupBy(\"race\").agg(avg(\"enemies_defeated\").alias(\"avg_enemies\")).orderBy(\"avg_enemies\", ascending=False))\n",
    "res1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f72a4902-4e0c-4814-95ae-b18d9bc84e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+--------------+\n",
      "|submarine_id|total_missions|total_warheads|\n",
      "+------------+--------------+--------------+\n",
      "|      SUB-01|             3|            16|\n",
      "|      SUB-03|             1|             6|\n",
      "|      SUB-02|             1|             5|\n",
      "+------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Problem 3: Submarine Nuclear Warhead Deployment \n",
    "Problem Description:\n",
    "You’re the data officer aboard the USS Nautilus, a nuclear submarine tasked with managing warhead deployments. \n",
    "The admiral needs a report to evaluate launch efficiency across missions.\n",
    "Select the submarine ID, mission name, and warheads launched, filter for missions where more than 3 warheads\n",
    "were launched successfully, group by submarine ID to count the total missions and sum the warheads launched,\n",
    "and order by total warheads launched in descending order. This analysis will determine which submarines are ready \n",
    "for the next high-stakes operation in the Pacific.\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import count, sum\n",
    "\n",
    "data = [\n",
    "    (\"SUB-01\", \"Pacific Strike\", 5, \"Success\"),\n",
    "    (\"SUB-02\", \"Atlantic Surge\", 2, \"Failure\"),#\n",
    "    (\"SUB-01\", \"Arctic Blitz\", 4, \"Success\"),\n",
    "    (\"SUB-03\", \"Indian Ocean\", 6, \"Success\"),\n",
    "    (\"SUB-02\", \"Pacific Strike\", 3, \"Success\"),#\n",
    "    (\"SUB-01\", \"Coral Sea\", 7, \"Success\"),\n",
    "    (\"SUB-03\", \"Arctic Blitz\", 1, \"Failure\"),#\n",
    "    (\"SUB-02\", \"Bering Strait\", 5, \"Success\")\n",
    "]\n",
    "\n",
    "columns = [\"submarine_id\", \"mission_name\", \"warheads_launched\", \"status\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "res = (df.select(\"submarine_id\", \"mission_name\", \"warheads_launched\").filter((df.warheads_launched > 3) & (df.status == \"Success\"))\n",
    "      .groupBy(\"submarine_id\").agg(count(\"mission_name\").alias(\"total_missions\"), sum(\"warheads_launched\").alias(\"total_warheads\"))\n",
    "    .orderBy(\"total_warheads\", ascending = False))\n",
    "res.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
